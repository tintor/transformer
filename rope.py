from typing import Tuple
import torch

def precompute_freqs_cis(dim: int, seq_len: int, theta: float=10000.0) -> torch.Tensor:
    # Computing Theta value for each dim pair which is dim/2
    device = 'cuda'
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device='cuda')[:(dim//2)].float()/dim))

    # Computing range of positions(m) in the sequence
    t = torch.arange(seq_len, dtype=torch.float32, device=device)

    # freqs gives all the Theta value range for all the position of tokens in the sequence
    freqs = torch.outer(t, freqs).to(device)

    # This is the rotation matrix which needs to be converted to Polar form in order to perform rotation to the embedding
    return torch.polar(torch.ones_like(freqs).to(device), freqs).to(device)


def _reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor) -> torch.Tensor:
    ndim = x.ndim
    assert 0 <= 1 < ndim
    assert freqs_cis.shape == (x.shape[1], x.shape[-1]), "the last two dimension of freqs_cis, x must match"
    shape = [d if i == 1 or i == ndim-1 else 1 for i, d in enumerate(x.shape)]
    return freqs_cis.view(*shape)


def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
    device = 'cuda'
    # Applying rotary positional encoding to both query and key embedding together
    # First: The last dimension of xq and xk embedding needs to be reshaped to make it a pair. As rotation matrix is applied to each pair of dim.
    # Next: convert both xq and xk to complex number as the rotation matrix is only applicable to complex number
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2)).to(device)    #xq_:[bsz, seq_len, n_heads, head_dim/2]
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2)).to(device)    #xk_:[bsz, seq_len, n_heads, head_dim/2]

    # The rotation matrix(freqs_cis) dimensions across seq_len(dim=1) and head_dim(dim=3) should match with the embedding
    # Also, the shape freqs_cis should be the same with xq and xk, hence change the shape of freqs_cis:[seq_len,head_dim] -> freqs_cis:[1,seq_len,1,head_dim]
    freqs_cis = _reshape_for_broadcast(freqs_cis, xq_)

    # Finally, perform rotation operation by multiplying with freqs_cis.
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3).to(device).type_as(xq) #xq_out:[bsz, seq_len, n_heads, head_dim]
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3).to(device).type_as(xk) #xk_out:[bsz, seq_len, n_heads, head_dim]
    return xq_out, xk_out
